{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: Bag-of-Words Model\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/bow_guide.ipynb](guides/bow_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bow_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 2) (7600, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 2), (760, 2))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "    \"\"\" Preprocess the dataset \n",
    "\n",
    "    Operations:\n",
    "    - Map the label to the corresponding category\n",
    "    - Filter out the labels not in the label_map\n",
    "    - Sample a fraction of the dataset (stratified by label)\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to preprocess\n",
    "    - frac (float): The fraction of the dataset to sample in each category\n",
    "    - label_map (dict): A mapping of the original label to the new label\n",
    "    - seed (int): The random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "del train\n",
    "del test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840,) (360,) (840,) (360,)\n"
     ]
    }
   ],
   "source": [
    "(   X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val\n",
    "\n",
    ") = train_test_split(train_df[\"text\"], train_df[\"label\"], test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countvectorizer\n",
    "cv = CountVectorizer(\n",
    "    max_features=None,      # All words are considered since computational efficiency is not a concern\n",
    "    min_df=1,               # Ignore very rare words (noise reduction) --> Best results are obtained when min_df=1\n",
    "    max_df=10,              # Ignore very common words (noise reduction) --> Best results are obtained when max_df=10\n",
    "    ngram_range=(1,2),      # Consider both single words and bigrams \n",
    "    stop_words=None)        # Do not remove stop words \n",
    "X_train_vectorized = cv.fit_transform(X_train)\n",
    "\n",
    "X_train_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       1.00      1.00      1.00       206\n",
      "      Sports       1.00      1.00      1.00       209\n",
      "    Business       1.00      1.00      1.00       208\n",
      "    Sci/Tech       1.00      1.00      1.00       217\n",
      "\n",
      "    accuracy                           1.00       840\n",
      "   macro avg       1.00      1.00      1.00       840\n",
      "weighted avg       1.00      1.00      1.00       840\n",
      "\n",
      "Performance on the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.80      0.70      0.75        94\n",
      "      Sports       0.75      0.78      0.76        91\n",
      "    Business       0.84      0.86      0.85        92\n",
      "    Sci/Tech       0.75      0.81      0.78        83\n",
      "\n",
      "    accuracy                           0.79       360\n",
      "   macro avg       0.79      0.79      0.79       360\n",
      "weighted avg       0.79      0.79      0.79       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression() # Note that we can set hyperparameters here\n",
    "\n",
    "lr_clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "X_val_vectorized = cv.transform(X_val) # note that we use transform here, not fit_transform\n",
    "\n",
    "y_pred = lr_clf.predict(X_val_vectorized)\n",
    "\n",
    "print(\"Performance on the training set:\")\n",
    "print(classification_report(y_train, lr_clf.predict(X_train_vectorized), target_names=label_map.values()))\n",
    "\n",
    "print(\"Performance on the validation set:\")\n",
    "print(classification_report(y_val, y_pred, target_names=label_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.78      0.74      0.76       190\n",
      "      Sports       0.82      0.82      0.82       190\n",
      "    Business       0.90      0.91      0.90       190\n",
      "    Sci/Tech       0.81      0.85      0.83       190\n",
      "\n",
      "    accuracy                           0.83       760\n",
      "   macro avg       0.83      0.83      0.83       760\n",
      "weighted avg       0.83      0.83      0.83       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_vectorized = cv.transform(test_df[\"text\"])\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(classification_report(test_df[\"label\"], lr_clf.predict(test_df_vectorized), target_names=label_map.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
